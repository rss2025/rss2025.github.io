---
layout: paper
title: "Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy"
invisible: true
prev_id: "S11.2"
next_id: "S11.4"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Jiayi Chen, Yubin Ke, Lin Peng, He Wang</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf-modern">
  <div class="paper-menu-icon">
    <a href="https://www.roboticsproceedings.org/rss25/p149.pdf" title="Download PDF" target="_blank">
      <i class="fa fa-file-pdf-o"></i><br>
      <span class="paper-menu-label">PDF</span>
    </a>
  </div>
</div>

### Paper ID S11.3
{: style="margin-top: 10px; text-align: center;" }

### [Session 11. Manipulation II]({{ site.baseurl }}/program/papersession?session=11.%20Manipulation%20II)
{: style="text-align: center;" }

<b style="color: black;">Abstract: </b>Generalizable dexterous grasping is a fundamental skill for intelligent robots. To develop such skills, a large-scale, high-quality, and diverse dataset of robotic dexterous grasps—covering the GRASP taxonomy—is essential but extremely challenging to collect. Previous dexterous grasp synthesis methods are often limited to specific grasp types or object categories, and tend to suffer from issues like penetration and unnatural poses. In this work, we address these challenges by proposing an efficient method capable of synthesizing physically plausible, contact-rich, and penetration-free dexterous grasps for any grasp type, object, and articulated robotic hand. Starting from only one human-annotated template per hand and grasp type, our pipeline first uses a lightweight global alignment stage to optimize the object pose and then a simulation-based local refinement stage to adjust the hand pose. Next, to validate the synthesized grasps, we introduce a contact-aware control strategy that applies desired forces to each contact point on the object. The validated grasps can further enrich the grasp template library and facilitate future synthesis. Experimental results demonstrate the superiority of our pipeline over existing grasp synthesis approaches for both fingertip and other grasp types. Furthermore, using our synthesized grasps, we show that a type-conditional generative model can successfully learn and perform the desired grasp type in both simulation and the real world.
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/S11.2/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fa fa-chevron-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fa fa-list"></i><br>
        <span class="paper-menu-label">All Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/S11.4/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fa fa-chevron-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
