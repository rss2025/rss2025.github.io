---
layout: paper
title: "Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success"
invisible: true
prev_id: "S2.7"
next_id: "S2.9"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Moo Jin Kim, Chelsea Finn, Percy Liang</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf-modern">
  <div class="paper-menu-icon">
    <a href="https://www.roboticsproceedings.org/rss25/p707.pdf" title="Download PDF" target="_blank">
      <i class="fa fa-file-pdf-o"></i><br>
      <span class="paper-menu-label">PDF</span>
    </a>
  </div>
</div>

### Paper ID S2.8
{: style="margin-top: 10px; text-align: center;" }

### [Session 2. VLA Models]({{ site.baseurl }}/program/papersession?session=2.%20VLA%20Models)
{: style="text-align: center;" }

<b style="color: black;">Abstract: </b>Recent vision-language-action models (VLAs), which build upon pretrained vision-language models and leverage diverse robot datasets, have demonstrated strong task execution, language-following ability, and out-of-distribution generalization. Despite their success, VLAs struggle with novel robot setups and must be fine-tuned to achieve optimal performance. However, existing fine-tuning methods yield suboptimal speed and task performance, and systematic investigations of alternative adaptation strategies and controlled evaluations of their effects remain largely underexplored. In this work, we conduct a comprehensive study of adaptation design choices for the recently released OpenVLA model, examining different action decoding schemes, action representations, and learning objectives for fine-tuning. Based on our findings, we propose OpenVLA-OFT, an instantiation of our Optimized Fine-Tuning recipe that integrates parallel decoding, action chunking, continuous action representations, and a simple L1 regression-based learning objective to altogether improve inference efficiency, policy performance, and model input/output flexibility. OpenVLA-OFT sets a new state of the art on the LIBERO simulation benchmark, significantly boosting OpenVLA's average success rate across four task suites from 76% to 97% while increasing action generation throughput by 26Ã—. In real-world evaluation, OpenVLA-OFT successfully performs dexterous, high-frequency control tasks on a dual-arm ALOHA robot and matches or outperforms strong imitation learning methods trained from scratch as well as other fine-tuned VLAs. We will release code for the optimized fine-tuning recipe, pretrained model checkpoints, and datasets upon publication.
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/S2.7/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fa fa-chevron-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fa fa-list"></i><br>
        <span class="paper-menu-label">All Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/S2.9/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fa fa-chevron-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
