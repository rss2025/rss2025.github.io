---
layout: paper
title: "PartInstruct: Part-level Instruction Following for Fine-grained Robot Manipulation"
invisible: true
prev_id: "147"
next_id: "149"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Yifan Yin, Zhengtao Han, Shivam Aarya, Shuhang Xu, Jianxin Wang, Jiawei Peng, Angtian Wang, Alan Yuille, Tianmin Shu</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf-modern">
  <div class="paper-menu-icon">
    <a href="https://www.roboticsproceedings.org/rss21/p148.pdf" title="Download PDF" target="_blank">
      <i class="fa fa-file-pdf-o"></i><br>
      <span class="paper-menu-label">PDF</span>
    </a>
  </div>
</div>

### Paper ID 148
{: style="margin-top: 10px; text-align: center;" }

### [Session 16. Manipulation III]({{ site.baseurl }}/program/papersession?session=16.%20Manipulation%20III)
{: style="text-align: center;" }

<b style="color: black;">Abstract: </b>Fine-grained robot manipulation, such as lifting and rotating a bottle to display the label on the cap, requires robust reasoning about object parts and their relationships with intended tasks. Despite recent advances in training general-purpose robot manipulation policies guided by language instructions, there is a notable lack of large-scale datasets for fine-grained manipulation tasks with part-level instructions and diverse 3D object instances annotated with part-level labels. In this work, we introduce PartInstruct, the first large-scale benchmark for both training and evaluating fine-grained robot manipulation models using part-level instructions. PartInstruct comprises 513 object instances across 14 categories, each annotated with part-level information, and 1302 fine-grained manipulation tasks organized into 16 task classes. We generated a training set that includes over 10,000 expert demonstrations synthesized in a 3D simulator, each annotated with an overall task instruction, a chain of basic part-based skill instructions, and ground-truth 3D information about the object and its parts. Additionally, we designed a comprehensive test suite to evaluate the generalizability of learned policies across new states, objects, and tasks. We evaluated several state-of-the-art vision-language policy learning methods for robot manipulation on our benchmark. The experimental results reveal that current models struggle to robustly ground part concepts in 3D vision and motion planning, and face challenges when manipulating object parts in long-horizon tasks.
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/147/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fa fa-chevron-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fa fa-list"></i><br>
        <span class="paper-menu-label">Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/149/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fa fa-chevron-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
