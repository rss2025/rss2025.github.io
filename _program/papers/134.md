---
layout: paper
title: "One-Shot Imitation Learning with Invariance Matching for Robotic Manipulation"
invisible: true
---
<div class="paper-authors">
<div class="paper-author-box">
    <div class="paper-author-name">Xinyu Zhang, Abdeslam Boularias</div>
    <div class="paper-author-uni"></div>
</div>

</div><div class="paper-pdf">
                <div> <a href="https://www.roboticsproceedings.org/rss20/p134.pdf"><img src="{{ site.baseurl }}/images/paper_link.png" alt="Paper Website" width = "33"  height = "40"/></a> </div>
                </div>

### Paper ID 134
{: style="margin-top: 10px; text-align: center;"}

### [Session 16. Manipulation]({{ site.baseurl }}/program/papersession?session=16.%20Manipulation)
{: style="text-align: center;"}

#### Poster Session day 4 (Friday, July 19)
{: style="margin-top: 10px; color: #555555; text-align: center;"}

<b style="color: black;">Abstract: </b>Learning a single universal policy that can perform
 a diverse set of manipulation tasks is a promising new direction
 in robotics. However, existing techniques are limited to learning
 policies that can only perform tasks that are encountered during
 training, and require a large number of demonstrations to learn
 new tasks. Humans, on the other hand, often can learn a new
 task from a single unannotated demonstration. In this work,
 we propose the Invariance-Matching One-shot Policy Learning
 (IMOP) algorithm. In contrast to the standard practice of learning
 the end-effector’s pose directly, IMOP first learns invariant regions
 of the state space for a given task, and then computes the end-
 effector’s pose through matching the invariant regions between
 demonstrations and test scenes. Trained on the 18 RLBench
 tasks, IMOP achieves a success rate that outperforms the state-
 of-the-art consistently, by 4.5% on average over the 18 tasks.
 More importantly, IMOP can learn a novel task from a single
 unannotated demonstration, and without any fine-tuning, and
 achieves an average success rate improvement of 11.5% over the
 state-of-the-art on 22 novel tasks selected across nine categories.
 IMOP can also generalize to new shapes and learn to manipulate
 objects that are different from those in the demonstration. Further,
 IMOP can perform one-shot sim-to-real transfer using a single
 real-robot demonstration.
{: style="color:gray; font-size: 120%; text-align: justified;"}


<div class="paper-menu">
<a href="{{ site.baseurl }}/program/papers/133/"> <img src="{{ site.baseurl }}/images/previous_paper_icon.png" alt="Previous Paper" title="Previous Paper"/> </a>
<a href="{{ site.baseurl }}/program/papers"><img src="{{ site.baseurl }}/images/overview_icon.png" alt="All Papers" title="All Papers"/> </a>
<img src="{{ site.baseurl }}/images/blank_icon.png" alt="End of Program" title="End of Program"/> 

</div>
