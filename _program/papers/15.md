---
layout: paper
title: "Unified World Models: Coupling Video and Action Diffusion for Pretraining on Large Robotic Datasets"
invisible: true
prev_id: "14"
next_id: "16"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Chuning Zhu, Raymond Yu, Siyuan Feng, Benjamin Burchfiel, Paarth Shah, Abhishek Gupta</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf">
  <div>
    <a href="https://www.roboticsproceedings.org/rss21/p015.pdf" title="Download PDF" target="_blank">
      <img src="{{ site.baseurl }}/images/paper_link_cardinal_red.png" alt="Paper PDF" width="33" height="40" />
    </a>
  </div>
</div>

### Paper ID 15
{: style="margin-top: 10px; text-align: center;" }

### [Session 2. VLA Models]({{ site.baseurl }}/program/papersession?session=2.%20VLA%20Models)
{: style="text-align: center;" }

#### Poster Session (Day 1): Saturday, June 21, 6:30-8:00 PM
{: style="margin-top: 10px; color: #555555; text-align: center;" }

<b style="color: black;">Abstract: </b>Imitation learning has emerged as a promising approach towards building generalist robots. However, the reliance on high-quality expert demonstrations poses a challenge in scaling imitation learning for large-scale robot foundation models. On the other hand, large amounts of video data depicting a wide range of environments and diverse behaviors are readily available. This data provides a rich source of information about real-world dynamics and agent-environment interactions. Leveraging this data efficiently for robotics, however, is difficult due to the lack of action annotation necessary for current imitation learning methods. In this work, we present Unified World Models, a framework that allows for leveraging video data for policy learning. Specifically, a UWM integrates an action diffusion process and a video diffusion process within a unified transformer architecture, where independent diffusion timesteps govern each modality. By controlling each diffusion timestep, UWMs can flexibly generate samples from the forward dynamics, the inverse dynamics, as well as marginal and joint distributions. Through simulated and real-world experiments, we show that: (1) UWMs can effectively be used as a policy class for behavior cloning, achieving comparable performance to state-of-the-art behavior cloning methods, (2) UWMs enable efficient pretraining on large-scale multitask robot datasets, where finetuned policies outperform baselines in terms of generalization and robustness and (3) UWMs naturally enable learning from action-free video data through independent control of modality-specific diffusion timesteps, further improving the performance of finetuned policies. Our results suggest that UWMs offer a promising step toward harnessing large, heterogeneous datasets for scalable robot learning.
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/14/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fas fa-list"></i><br>
        <span class="paper-menu-label">Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/16/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fas fa-arrow-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
