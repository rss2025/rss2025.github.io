---
layout: paper
title: "CREStE: Scalable Mapless Navigation with Internet Scale Priors and Counterfactual Guidance"
invisible: true
prev_id: "135"
next_id: "137"
---
<div class="paper-authors">
  <div class="paper-author-box">
    <div class="paper-author-name">Arthur Zhang, Harshit Sikchi, Joydeep Biswas, Amy Zhang</div>
    <div class="paper-author-uni"></div>
  </div>
</div>

<div class="paper-pdf-modern">
  <div class="paper-menu-icon">
    <a href="https://www.roboticsproceedings.org/rss21/p136.pdf" title="Download PDF" target="_blank">
      <i class="fa fa-file-pdf-o"></i><br>
      <span class="paper-menu-label">PDF</span>
    </a>
  </div>
</div>

### Paper ID 136
{: style="margin-top: 10px; text-align: center;" }

### [Session 15. Navigation]({{ site.baseurl }}/program/papersession?session=15.%20Navigation)
{: style="text-align: center;" }

<b style="color: black;">Abstract: </b>We address the long-horizon mapless navigation problem: enabling robots to traverse novel environments without relying on high-definition maps or precise waypoints that specify exactly where to navigate. Two major challenges arise: (1) learning robust, generalizable perceptual representations of the environment—where it is impossible to pre-enumerate all relevant factors and ensure robustness to perceptual aliasing—and (2) planning human-aligned navigation paths using these learned features. Existing solutions often struggle to generalize due to their reliance on: (a) hand-curated object lists, which overlook new, unforeseen factors; (b) end-to-end learning of navigation-relevant features, which is constrained by the limited availability of real robot data; (c) large sets of expert demonstrations, which provide insufficient guidance on the most critical perceptual cues; or (d) handcrafted reward functions for learning, which are difficult to design and adapt for new scenarios. To overcome these limitations, we propose CREStE, a framework for representation and policy learning that does not require large-scale robot datasets or manually specified feature sets. First, CREStE  leverages visual foundation models trained on internet-scale data to learn continuous bird’s-eye-view representations capturing elevation, semantics, and instance-level features. Second, it incorporates a counterfactual-based loss and active learning procedure to focus on the perceptual cues that matter most by querying humans for counterfactual trajectory annotations in challenging scenes. We evaluate CREStE in kilometer-scale navigation tasks across six distinct urban environments. Our experiments demonstrate that CREStE achieves more efficient navigation and requires fewer human interventions than existing approaches, showcasing its robustness and effectiveness for long-horizon mapless navigation.
{: style="color:gray; font-size: 120%; text-align: justified;" }

<div class="paper-menu">
  <div class="paper-menu-inner">
    <a href="{{ site.baseurl }}/program/papers/135/" title="Previous Paper">
            <div class="paper-menu-icon">
                <i class="fa fa-chevron-left"></i><br>
                <span class="paper-menu-label">Back</span>
            </div>
        </a>
    <a href="{{ site.baseurl }}/program/papers" title="All Papers">
      <div class="paper-menu-icon">
        <i class="fa fa-list"></i><br>
        <span class="paper-menu-label">Papers</span>
      </div>
    </a>
    <a href="{{ site.baseurl }}/program/papers/137/" title="Next Paper">
            <div class="paper-menu-icon">
                <i class="fa fa-chevron-right"></i><br>
                <span class="paper-menu-label">Next</span>
            </div>
        </a>
  </div>
</div>
